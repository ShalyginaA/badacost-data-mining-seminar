{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BAdaCost:\n",
    "    def __init__(self,optimizer, n_iters, learning_rate, C, eps):\n",
    "        self.optimizer = optimizer #weak learners\n",
    "        self.n_iters = n_iters #number of iterations\n",
    "        self.lr = learning_rate #learning rate\n",
    "        self.weights = np.zeros(n_iters) #weights of weak learners\n",
    "        self.weak_learners = [0]*n_iters #weak learners\n",
    "        self.Cprime = C - np.diag(np.array(np.sum(C,axis=1).flatten())[0]) #modified cost matrix\n",
    "        self.n_classes = C.shape[1] #number of classes\n",
    "        self.margin = ((-1/(self.n_classes-1))*np.ones([self.n_classes,self.n_classes]) \n",
    "                 + (self.n_classes/(self.n_classes-1))*np.eye(self.n_classes)) #margin vector\n",
    "        self.eps = eps \n",
    "        \n",
    "    def translate_to_cost_matrix(self,C2,beta):\n",
    "        K = C2.shape[0]\n",
    "        Cexp = np.exp(beta*C2)\n",
    "        for j in range(K):\n",
    "            Cexp[j,:] -= Cexp[j,j]*np.ones(K)\n",
    "        return Cexp\n",
    "    \n",
    "    def compute_weak_learner_weight(self,C_star, W, pred, y):\n",
    "    #Computes  Weak Learner weight (\\alpha) in order to minimize the \n",
    "    #cost sensitive loss function.\n",
    "        K = C_star.shape[0]\n",
    "        WeightsSum = np.zeros([K,K])\n",
    "        for i in range(K):\n",
    "            for j in range(K):\n",
    "                y1 = []\n",
    "                pred1 = []\n",
    "                for k,z in zip(y,pred):\n",
    "                    if k == i:     \n",
    "                        y1.append(1)\n",
    "                    else:\n",
    "                        y1.append(0)\n",
    "                    if z == j:\n",
    "                        pred1.append(1)\n",
    "                    else:\n",
    "                        pred1.append(0)\n",
    "                predicted_as_j_being_i = y1 and pred1\n",
    "                WeightsSum[i,j] = np.dot(W,predicted_as_j_being_i)\n",
    "        alpha0 = 1.0\n",
    "        alpha = scipy.optimize.fmin(lambda x: self.cost_sensitive_loss_function(x,C_star,WeightsSum),\n",
    "                                    x0=alpha0,disp=0)\n",
    "        return alpha \n",
    "    \n",
    "    def cost_sensitive_loss_function(self,alpha,C_star,WeightsSum):\n",
    "        #Loss function computation for a given\n",
    "        #alpha (weak learner weight in the CostSAMME algorithm).\n",
    "        K = C_star.shape[0]\n",
    "        func_value = 0\n",
    "        for i in range(K):\n",
    "            for j in range(K):\n",
    "                func_value += WeightsSum[i,j]*np.exp(alpha*C_star[i,j])\n",
    "        return func_value \n",
    "    \n",
    "    def compute_weak_learner_cost(self,pred_wl, y, C2, beta, W):\n",
    "        cost = 0.0\n",
    "        for i in range(len(pred_wl)):\n",
    "            cost += W[i]*np.exp(beta*C2[y[i],pred_wl[i]])\n",
    "        return cost\n",
    "        \n",
    "    def fit(self,X,y):\n",
    "        N = X.shape[0]\n",
    "        sample_weights = (1/N)*np.ones(N)\n",
    "        C2 = self.Cprime * self.margin\n",
    "        for i in range(self.n_iters):\n",
    "            beta = 1\n",
    "            c = 100000 #inf\n",
    "            delta_c = 100000 #inf\n",
    "            while delta_c >= self.eps:\n",
    "                #print(beta)\n",
    "                C_wl = self.translate_to_cost_matrix(C2,beta)\n",
    "                G = self.train_multiclass_cost_sensitive_WL(X,y,sample_weights,C_wl)\n",
    "                wl_preds = G.predict(X)\n",
    "                beta = self.compute_weak_learner_weight(C2,sample_weights,wl_preds,y)[0]                \n",
    "                c_new = self.compute_weak_learner_cost(wl_preds,y,C2,beta,sample_weights)\n",
    "                delta_c = c-c_new\n",
    "                if beta <= 0:\n",
    "                    break\n",
    "            self.weak_learners[i] = G\n",
    "            self.weights[i] = beta\n",
    "            beta = self.lr*beta\n",
    "            \n",
    "            # update sample weights: only the right classified samples changes the\n",
    "            #weight\n",
    "            for j in range(X.shape[1]):\n",
    "                exp_j = C2[y[j],wl_preds[j]]\n",
    "                sample_weights[j] *= np.exp(beta*exp_j)\n",
    "            sample_weights /= np.sum(sample_weights)\n",
    "            \n",
    "    def weak_learner_prediction(self,weak_learner,X):\n",
    "        probs = weak_learner.predict_proba(X)\n",
    "        prediction = []\n",
    "        n = probs.shape[0]\n",
    "        for i in range(n):\n",
    "            a = np.dot(self.Cprime,probs[i])\n",
    "            a_min = np.argmin(a,axis=1)\n",
    "            prediction.append(a_min)\n",
    "        return np.array(prediction).reshape(1,n)[0]\n",
    "            \n",
    "                   \n",
    "    def predict(self,X):\n",
    "        n = X.shape[0]\n",
    "        M = len(self.weak_learners)\n",
    "        margin_vec = np.zeros([self.n_classes,n])\n",
    "        for i in range(len(self.weak_learners)):\n",
    "            #row vector with the labels\n",
    "            z = self.weak_learner_prediction(self.weak_learners[i],X)\n",
    "            for j in range(n):\n",
    "                margin_vec[:,j] += self.weights[i]*self.margin[:,z[j]]\n",
    "          \n",
    "        predicted = -self.Cprime*margin_vec\n",
    "        predicted = np.argmin(predicted.transpose(),axis=1)\n",
    "        return predicted.reshape(1,n)\n",
    "    \n",
    "    \n",
    "    def train_multiclass_cost_sensitive_WL(self,X,y,w,C_wl):\n",
    "        wl = self.optimizer\n",
    "        wl.fit(X,y)\n",
    "        return wl\n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = make_classification(n_samples = 100, n_features = 12, n_informative = 3,n_classes = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-3.4699708 , -0.71754178,  1.33363679, ..., -0.06187781,\n",
       "        -0.12723468,  0.34968287],\n",
       "       [-1.19784165, -0.38040367, -1.8263622 , ..., -0.13239404,\n",
       "         1.36320067,  0.94631531],\n",
       "       [ 0.53577625,  0.27599987, -1.42072552, ..., -1.03480589,\n",
       "        -2.42279669,  0.35709161],\n",
       "       ...,\n",
       "       [-0.33420442, -1.86056807,  0.10800062, ...,  0.21866531,\n",
       "         1.84939184,  1.45391778],\n",
       "       [-0.17836352,  1.45327679, -1.63503408, ...,  0.8129559 ,\n",
       "        -1.48725447,  1.00878937],\n",
       "       [-0.25577954,  1.64714689,  1.32116101, ..., -1.94656682,\n",
       "        -1.90090206,  0.28571722]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 2, 0, 1, 0, 2, 0, 0, 0, 2, 0, 1, 2, 2, 1, 2, 2, 1, 1, 1, 2,\n",
       "       1, 2, 0, 0, 2, 2, 2, 1, 0, 2, 1, 0, 2, 0, 2, 1, 2, 0, 2, 1, 1, 2,\n",
       "       0, 2, 0, 2, 0, 0, 1, 0, 1, 1, 0, 2, 2, 0, 1, 1, 0, 2, 0, 2, 0, 0,\n",
       "       1, 0, 1, 2, 1, 0, 1, 1, 1, 2, 1, 2, 1, 1, 0, 2, 0, 0, 2, 2, 1, 1,\n",
       "       1, 2, 2, 2, 1, 0, 0, 0, 1, 0, 1, 1])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "tree = DecisionTreeClassifier(max_depth=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = np.matrix([[0,1,1],[1,0,1],[1,1,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "bada = BAdaCost(tree,10,0.001,C,0.000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "bada.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0, 0, 2, 0, 1, 2, 2, 0, 0, 0, 2, 0, 1, 2, 2, 1, 2, 2, 1, 1, 1,\n",
       "         2, 1, 2, 0, 0, 2, 2, 2, 1, 0, 2, 1, 0, 2, 0, 2, 1, 2, 0, 2, 1,\n",
       "         1, 2, 0, 2, 0, 2, 0, 0, 1, 0, 1, 1, 0, 2, 2, 0, 1, 1, 0, 2, 0,\n",
       "         2, 0, 0, 1, 0, 1, 2, 1, 0, 1, 1, 1, 2, 1, 2, 1, 1, 0, 2, 0, 0,\n",
       "         2, 2, 1, 1, 1, 2, 2, 2, 2, 0, 0, 0, 1, 0, 1, 1]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bada.predict(X)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
