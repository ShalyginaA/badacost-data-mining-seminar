{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BAdaCost:\n",
    "    def __init__(optimizer, n_iters, learning_rate, C, eps):\n",
    "        self.optimizer = optimizer #weak learners\n",
    "        self.n_iters = n_iters #number of iterations\n",
    "        self.lr = learning_rate #learning rate\n",
    "        #self.sample_weights = [0]*n_iters\n",
    "        self.weights = np.zeros(n_iters) #weights of weak learners\n",
    "        self.weak_learners = [0]*n_iters #weak learners\n",
    "        self.Cprime = C - np.diag(np.array(np.sum(C,axis=1).flatten())[0]) #modified cost matrix\n",
    "        self.n_classes = C.shape[1] #number of classes\n",
    "        self.margin = ((-1/(self.n_classes-1))*np.ones(self.n_classes,self.n_classes) \n",
    "                 + (self.n_classes/(self.n_classes-1))*np.eye(self.n_classes)) #margin vector\n",
    "        self.eps = eps \n",
    "        \n",
    "    def translate_to_cost_matrix(self,C2,beta):\n",
    "        K = C2.shape[0]\n",
    "        Cexp = np.exp(beta*C2)\n",
    "        for j in range(K):\n",
    "            Cexp[j,:] -= Cexp[j,j]*np.ones(K)\n",
    "        return Cexp\n",
    "    \n",
    "    def compute_weak_learner_weight(self,C_star, W, pred, y):\n",
    "    #Computes  Weak Learner weight (\\alpha) in order to minimize the \n",
    "    #cost sensitive loss function.\n",
    "        K = C_star.shape[0]\n",
    "        WeightsSum = np.zeros([K,K])\n",
    "        for i in range(K):\n",
    "            for j in range(K):\n",
    "                y1 = []\n",
    "                pred1 = []\n",
    "                for k,z in zip(y,pred):\n",
    "                    if k == i:     \n",
    "                        y1.append(1)\n",
    "                    else:\n",
    "                        y1.append(0)\n",
    "                    if z == j:\n",
    "                        pred1.append(1)\n",
    "                    else:\n",
    "                        pred1.append(0)\n",
    "                predicted_as_j_being_i = y1 and pred1\n",
    "                WeightsSum[i,j] = np.dot(W,predicted_as_j_being_i)\n",
    "        alpha0 = 1.0\n",
    "        alpha = scipy.optimize.fmin(lambda x: cost_sensitive_loss_function(x,C_star,WeightsSum),x0=alpha0)\n",
    "        return alpha \n",
    "    \n",
    "    def cost_sensitive_loss_function(self,alpha,C_star,WeightsSum):\n",
    "        #Loss function computation for a given\n",
    "        #alpha (weak learner weight in the CostSAMME algorithm).\n",
    "        K = C_star.shape[0]\n",
    "        func_value = 0\n",
    "        for i in range(K):\n",
    "            for j in range(K):\n",
    "                func_value += WeightsSum[i,j]*np.exp(alpha*C_star[i,j])\n",
    "        #deriv_value = 0\n",
    "        #for i in range(K):\n",
    "        #    for j in range(K):\n",
    "         #       deriv_value += WeightsSum[i,j]*C_star[i,j]*np.exp(alpha*C_star[i,j])\n",
    "        return func_value # deriv_value\n",
    "    \n",
    "    def compute_weak_learner_cost(self,pred_wl, y, C2, beta, W):\n",
    "        cost = 0\n",
    "        for i in range(len(pred_wl)):\n",
    "            cost += W[i]*np.exp(beta*C2[y[i]],pred_wl[i])\n",
    "        return cost\n",
    "        \n",
    "    def train(self,X,y):\n",
    "        N = X.shape[0]\n",
    "        sample_weights = (1/N)*np.ones(N)\n",
    "        C2 = self.Cprime * self.margin\n",
    "        for i in range(self.n_iters):\n",
    "            beta = 1\n",
    "            c = 100000 #inf\n",
    "            delta_c = 100000 #inf\n",
    "            while delta_c >= eps:\n",
    "                C_wl = translate_to_cost_matrix(C2,beta)\n",
    "                G = train_multiclass_cost_sensitive_WL(X,y,self.sample_weights,C_wl)\n",
    "                wl_preds = G.predict(X,y)\n",
    "                beta = compute_weak_learner_weight(C2,sample_weights,wl_preds,y)\n",
    "                c_new = compute_weak_learner_cost(wl_preds,y,C2,beta,W)\n",
    "                delta_c = c-c_new\n",
    "                if beta <= 0:\n",
    "                    break\n",
    "            self.weak_learners[i] = G\n",
    "            self.weights[i] = beta\n",
    "            beta = self.lr*beta\n",
    "            \n",
    "            # update sample weights: only the right classified samples changes the\n",
    "            #weight\n",
    "            for j in range(X.shape[1]):\n",
    "                exp_j = C2[y[j],wl_preds[j]]\n",
    "                sample_weights[j] *= np.exp(alpha*exp_j)\n",
    "            sample_weights /= np.sum(sample_weights)\n",
    "            \n",
    "    def predict(self,X):\n",
    "        n = X.shape[1]\n",
    "        margin_vec = np.zeros([self.n_classes,n])\n",
    "        for i in range(len(self.weak_learners)):\n",
    "            #row vector with the labels\n",
    "            z = self.weak_learners[i].predict(X)\n",
    "            for j in range(n):\n",
    "                margin_vec[:,j] += self.weights[i]*self.margin[:,z[j]]\n",
    "                \n",
    "        predicted = np.min(self.Cprime*margin_vec)\n",
    "        return predicted  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeakLearner:\n",
    "    def __init__()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
