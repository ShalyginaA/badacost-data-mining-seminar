{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BAdaCost:\n",
    "    def __init__(self,optimizer, n_iters, learning_rate, C, eps):\n",
    "        self.optimizer = optimizer #weak learners\n",
    "        self.n_iters = n_iters #number of iterations\n",
    "        self.lr = learning_rate #learning rate\n",
    "        #self.sample_weights = [0]*n_iters\n",
    "        self.weights = np.zeros(n_iters) #weights of weak learners\n",
    "        self.weak_learners = [0]*n_iters #weak learners\n",
    "        self.Cprime = C - np.diag(np.array(np.sum(C,axis=1).flatten())[0]) #modified cost matrix\n",
    "        self.n_classes = C.shape[1] #number of classes\n",
    "        self.margin = ((-1/(self.n_classes-1))*np.ones([self.n_classes,self.n_classes]) \n",
    "                 + (self.n_classes/(self.n_classes-1))*np.eye(self.n_classes)) #margin vector\n",
    "        self.eps = eps \n",
    "        \n",
    "    def translate_to_cost_matrix(self,C2,beta):\n",
    "        K = C2.shape[0]\n",
    "        Cexp = np.exp(beta*C2)\n",
    "        for j in range(K):\n",
    "            Cexp[j,:] -= Cexp[j,j]*np.ones(K)\n",
    "        return Cexp\n",
    "    \n",
    "    def compute_weak_learner_weight(self,C_star, W, pred, y):\n",
    "    #Computes  Weak Learner weight (\\alpha) in order to minimize the \n",
    "    #cost sensitive loss function.\n",
    "        K = C_star.shape[0]\n",
    "        WeightsSum = np.zeros([K,K])\n",
    "        for i in range(K):\n",
    "            for j in range(K):\n",
    "                y1 = []\n",
    "                pred1 = []\n",
    "                for k,z in zip(y,pred):\n",
    "                    if k == i:     \n",
    "                        y1.append(1)\n",
    "                    else:\n",
    "                        y1.append(0)\n",
    "                    if z == j:\n",
    "                        pred1.append(1)\n",
    "                    else:\n",
    "                        pred1.append(0)\n",
    "                predicted_as_j_being_i = y1 and pred1\n",
    "                WeightsSum[i,j] = np.dot(W,predicted_as_j_being_i)\n",
    "        alpha0 = 1.0\n",
    "        alpha = scipy.optimize.fmin(lambda x: self.cost_sensitive_loss_function(x,C_star,WeightsSum),\n",
    "                                    x0=alpha0,disp=0)\n",
    "        return alpha \n",
    "    \n",
    "    def cost_sensitive_loss_function(self,alpha,C_star,WeightsSum):\n",
    "        #Loss function computation for a given\n",
    "        #alpha (weak learner weight in the CostSAMME algorithm).\n",
    "        K = C_star.shape[0]\n",
    "        func_value = 0\n",
    "        for i in range(K):\n",
    "            for j in range(K):\n",
    "                func_value += WeightsSum[i,j]*np.exp(alpha*C_star[i,j])\n",
    "        #deriv_value = 0\n",
    "        #for i in range(K):\n",
    "        #    for j in range(K):\n",
    "         #       deriv_value += WeightsSum[i,j]*C_star[i,j]*np.exp(alpha*C_star[i,j])\n",
    "        return func_value # deriv_value\n",
    "    \n",
    "    def compute_weak_learner_cost(self,pred_wl, y, C2, beta, W):\n",
    "        cost = 0.0\n",
    "        for i in range(len(pred_wl)):\n",
    "            #print(W[i]*np.exp(beta*C2[y[i]],pred_wl[i]))\n",
    "            cost += W[i]*np.exp(beta*C2[y[i],pred_wl[i]])\n",
    "        return cost\n",
    "        \n",
    "    def train(self,X,y):\n",
    "        N = X.shape[0]\n",
    "        sample_weights = (1/N)*np.ones(N)\n",
    "        C2 = self.Cprime * self.margin\n",
    "        for i in range(self.n_iters):\n",
    "            beta = 1\n",
    "            c = 100000 #inf\n",
    "            delta_c = 100000 #inf\n",
    "            while delta_c >= self.eps:\n",
    "                #print(beta)\n",
    "                C_wl = self.translate_to_cost_matrix(C2,beta)\n",
    "                G = self.train_multiclass_cost_sensitive_WL(X,y,sample_weights,C_wl)\n",
    "                wl_preds = G.predict(X)\n",
    "                beta = self.compute_weak_learner_weight(C2,sample_weights,wl_preds,y)[0]                \n",
    "                c_new = self.compute_weak_learner_cost(wl_preds,y,C2,beta,sample_weights)\n",
    "                delta_c = c-c_new\n",
    "                if beta <= 0:\n",
    "                    break\n",
    "            self.weak_learners[i] = G\n",
    "            self.weights[i] = beta\n",
    "            beta = self.lr*beta\n",
    "            \n",
    "            # update sample weights: only the right classified samples changes the\n",
    "            #weight\n",
    "            for j in range(X.shape[1]):\n",
    "                exp_j = C2[y[j],wl_preds[j]]\n",
    "                sample_weights[j] *= np.exp(beta*exp_j)\n",
    "            sample_weights /= np.sum(sample_weights)\n",
    "            \n",
    "    def predict(self,X):\n",
    "        n = X.shape[0]\n",
    "        margin_vec = np.zeros([self.n_classes,n])\n",
    "        for i in range(len(self.weak_learners)):\n",
    "            #row vector with the labels\n",
    "            z = self.weak_learners[i].predict(X)\n",
    "            for j in range(n):\n",
    "                margin_vec[:,j] += self.weights[i]*self.margin[:,z[j]]\n",
    "                \n",
    "        predicted = self.Cprime*margin_vec\n",
    "        predicted = np.argmin(predicted.transpose(),axis=1)\n",
    "        return predicted.reshape(1,n)\n",
    "    \n",
    "    \n",
    "    #need to be modified for cost matrix. Now it just train simple \n",
    "    #sklearn DT\n",
    "    def train_multiclass_cost_sensitive_WL(self,X,y,w,C_wl):\n",
    "        dt = self.optimizer\n",
    "        dt.fit(X,y)\n",
    "        return dt\n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = np.matrix([[0,1,1],[1,0,1],[1,1,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0, 1, 1],\n",
       "        [1, 0, 1],\n",
       "        [1, 1, 0]])"
      ]
     },
     "execution_count": 442,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = make_classification(n_samples = 100, n_features = 12, n_informative = 3,n_classes = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.32176563, -0.90150066,  2.34690893, ..., -0.62424504,\n",
       "        -1.16779654,  0.70534362],\n",
       "       [ 1.44013455,  0.72988917,  1.46084451, ..., -0.44939692,\n",
       "         2.2025232 , -1.52531723],\n",
       "       [ 1.45762403,  0.24229071,  1.3919828 , ..., -1.60742855,\n",
       "         2.14277508,  0.26528909],\n",
       "       ...,\n",
       "       [-0.63233811, -0.46462202,  3.65400245, ...,  0.73575431,\n",
       "         2.61507739,  0.62686777],\n",
       "       [ 0.19168394, -0.65325177,  0.58608328, ..., -2.35920134,\n",
       "        -1.23408124,  1.72866854],\n",
       "       [-0.20696611,  1.91380807,  1.45391313, ..., -1.52543797,\n",
       "         1.58418909, -0.48763963]])"
      ]
     },
     "execution_count": 445,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [],
   "source": [
    "bada = BAdaCost(tree,10,0.001,C,0.000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [],
   "source": [
    "bada.train(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "         1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1,\n",
       "         0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,\n",
       "         1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0]])"
      ]
     },
     "execution_count": 448,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bada.predict(X)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
